# 概要

ボットとユーザのインタラクションを捉えるシステム（実装中）。

言語学・心理学の古典的理論をLLM APIを使用したアーキテクチャで実装し、マルチエージェント間での「当事者間評価による動的な合意形成」をシミュレートする。


# 設計指針


## 発話の評価指針

### 1. 説得性スコアの設計：

本システムでは、ユーザーの発話を「合理性（Rationality）」と「レトリック（Rhetoric）」の二軸で評価する。

#### 精密化見込みモデル (ELM) のインタラクティブ化:

先行研究: ELM は説得を「中心ルート（論理）」と「周辺ルート（印象）」の二重過程として捉える。

本システム: この二重過程と似た区分をターン制の対話に組み込み、エージェントの反応感度（Multiplier）を動的に変化させることで、インタラクティブなモデルへと適用。

#### Discourse Quality Index (DQI) との対比:

先行研究 (DQI): 通常、DQI は第三者の評価者が、議論の公益性や熟議の質を事後的に測定するための指標である。

本システム (Participant-Centric Evaluation): DQI とは対照的に、「議論の当事者（エージェント）」自身が評価者となる。第三者的な「公益性」を判定するのではなく、対立する参加者間で「個別の論理的整合性」と「受容性」を評価する。

#### グライスの会話の格率の統合:

先行研究: 協調の原理として、量・質・関係・様式の4つの会話の格率が提唱されている。

実装: 評価プロンプトに、これらに関係する要素（Relevance, Clarity, Factuality, Quantity等）を組み込み、数値化する。



### 2. ボットによる重み付けの違い

ボットのロールごとに、スコア変化における「合理性」「レトリック」の重視度合いを設定し、さらにユーザの発言の評価がプラスである場合の重み、マイナスである場合の重みを設定することで、「合理性を重視し、ミスに厳しい」「レトリックを重視し、甘い」「合理性を重視し、甘い」「レトリックを重視し、甘い」「評価をあまり変えない」「大きく評価を変えやすい」などのキャラクター付けを設定。議論展開に幅を持たせる。



### 3. 社会的信用の時間的変容

「信頼の構築プロセス」を、時間軸で分離して管理する。

* 社会的信用パラメータの二段階反映:

ユーザーの「いい人っぽさ」を、Primacy（初頭効果的な信頼）とConsolidation（事後定着的な信頼）に分離してスコアリングに反映させる。

これにより、単発のポライトネスではなく、議論や振る舞い方の蓄積（History）が現在の説得力にどう寄与するかをシミュレートする。



## Technical Implementation (技術的特徴)

* Proactive Initiation (口火切り): ユーザーが議題に対する具体的な回答（Answer/Stance）を保留した場合、エージェント A が論理的な口火を切り、議論の停滞を防ぐ。

* Dynamic Role Swapping: ユーザーがボットの提案に実質的な同意（AGREE）を示した際、陣営の役割定義を反転させ、議論の構図を動的に再構築する。

* Pragmatic Stance Detection: 形式的な構文（Proposition）ではなく、語用論的な機能（Pragmatics）からユーザーの真のスタンスを抽出する。



# 動かす

Discord



```

cd taiwa-bot

pip install -r requirements_discord.txt

python discord_bot.py

```





## Debug

### CLI（未検証）


```
python debug_cli.py
```

### Colab


"""
Google Colab等でのイベントループ競合対策コード例:

import nest_asyncio
nest_asyncio.apply()
import asyncio
# ↓ファイルをアップロードするならコメントアウト。
# 直にコード内容を貼り付けるなら不要。
# from debug_cli import main
await main()  # または asyncio.create_task(main())
"""


